{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM part-of-speech tagger with character-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2233e140990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = open(\"conll2000/train.txt\",\"r\") \n",
    "train_data = []\n",
    "word = []\n",
    "tag = []\n",
    "words = []\n",
    "tags = []\n",
    "for line in f:\n",
    "    line = line.lower().strip()\n",
    "    if line == '':\n",
    "        train_data.append((word,tag))\n",
    "        word = []\n",
    "        tag = []\n",
    "        sen = []\n",
    "    else:\n",
    "        word.append(line.split()[0])\n",
    "        tag.append(line.split()[1])\n",
    "        \n",
    "        words.append(line.split()[0])\n",
    "        tags.append(line.split()[1])\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"conll2000/test.txt\",\"r\") \n",
    "test_data = []\n",
    "word = []\n",
    "tag = []\n",
    "words = []\n",
    "tags = []\n",
    "for line in f:\n",
    "    line = line.lower().strip()\n",
    "    if line == '':\n",
    "        test_data.append((word,tag))\n",
    "        word = []\n",
    "        tag = []\n",
    "        sen = []\n",
    "    else:\n",
    "        word.append(line.split()[0])\n",
    "        tag.append(line.split()[1])\n",
    "        \n",
    "        words.append(line.split()[0])\n",
    "        tags.append(line.split()[1])\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "char_to_ix = {}\n",
    "\n",
    "for sent, tags in train_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "        for char in list(word):\n",
    "            if char not in char_to_ix:\n",
    "                char_to_ix[char] = len(char_to_ix)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "    \n",
    "for sent, tags in test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "        for char in list(word):\n",
    "            if char not in char_to_ix:\n",
    "                char_to_ix[char] = len(char_to_ix)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19460 44 54\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_ix)\n",
    "tag_size = len(tag_to_ix)\n",
    "char_size = len(char_to_ix)\n",
    "print(vocab_size,tag_size,char_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_EMBEDDING_DIM = 100\n",
    "CHAR_REP_DIM = 100\n",
    "WORD_EMBEDDING_DIM = 1000\n",
    "HIDDEN_DIM = 1000\n",
    "PRINT_EVERY = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix, is_train=False):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    # if not in training mode, return a volatile variable (no backward pass)\n",
    "    return autograd.Variable(tensor, requires_grad=False, volatile=not is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLvlRep(nn.Module):\n",
    "  \n",
    "    def __init__(self, embedding_dim, rep_dim, char_size):  \n",
    "        super(CharLvlRep, self).__init__()\n",
    "        self.rep_dim = rep_dim\n",
    "        self.char_embeddings = nn.Embedding(char_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, rep_dim)\n",
    "        \n",
    "        self.char_hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.rep_dim)), \n",
    "                autograd.Variable(torch.zeros(1, 1, self.rep_dim)))\n",
    "  \n",
    "    def forward(self, word, is_train=False):\n",
    "        embeds = self.char_embeddings(word)\n",
    "        \n",
    "        char_reps, _ = self.lstm(embeds.view(len(word), 1, -1), self.char_hidden)\n",
    "\n",
    "        final_char_rep = (char_reps[char_reps.size()[0]-1, :, :])\n",
    "\n",
    "        return final_char_rep\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, char_embedding_dim, char_rep_dim, char_size, word_embedding_dim, vocab_size, hidden_dim, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.char_rep_dim = char_rep_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #char_embed_dim, char_hidden_dim, char_dict_size\n",
    "        self.model_char = CharLvlRep(char_embedding_dim, char_rep_dim, char_size)\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "      \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_rep_dim, hidden_dim)\n",
    "      \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        self.word_hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)), \n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "\n",
    "    def forward(self, sentence, words, is_train=False):\n",
    "        word_embeds = self.word_embeddings(sentence)\n",
    "        word_embeds = word_embeds.view(len(sentence), 1, -1)\n",
    "      \n",
    "        char_reps = autograd.Variable(torch.zeros(len(words),1,self.char_rep_dim), volatile=not is_train)\n",
    "\n",
    "        for idx, char in enumerate(words):\n",
    "            char_reps[idx, :, :] = self.model_char(char, is_train)\n",
    "            \n",
    "        embeds_cat = torch.cat((word_embeds, char_reps), dim=2)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(embeds_cat, self.word_hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMTagger(CHAR_EMBEDDING_DIM,CHAR_REP_DIM, char_size, WORD_EMBEDDING_DIM, vocab_size, HIDDEN_DIM, tag_size)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/william.ardianto/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 0: loss = 0.0060774278827011585, accuracy = 1.0\n",
      "It 100: loss = 0.3171910047531128, accuracy = 0.8333333333333334\n",
      "It 200: loss = 0.8777325749397278, accuracy = 0.6774193548387096\n",
      "It 300: loss = 0.5802592635154724, accuracy = 0.8461538461538461\n",
      "It 400: loss = 1.1272722482681274, accuracy = 0.625\n",
      "It 500: loss = 0.6786277890205383, accuracy = 0.7631578947368421\n",
      "It 600: loss = 0.1586916744709015, accuracy = 1.0\n",
      "It 700: loss = 0.39816898107528687, accuracy = 0.9333333333333333\n",
      "It 800: loss = 0.07978050410747528, accuracy = 1.0\n",
      "It 900: loss = 0.8574633002281189, accuracy = 0.7619047619047619\n",
      "It 1000: loss = 0.31176719069480896, accuracy = 0.9130434782608695\n",
      "It 1100: loss = 0.39146387577056885, accuracy = 0.8125\n",
      "It 1200: loss = 0.42262521386146545, accuracy = 0.8529411764705882\n",
      "It 1300: loss = 0.7608103156089783, accuracy = 0.7647058823529411\n",
      "It 1400: loss = 0.18127794563770294, accuracy = 0.8333333333333334\n",
      "It 1500: loss = 0.02548396587371826, accuracy = 1.0\n",
      "It 1600: loss = 0.21616220474243164, accuracy = 0.9230769230769231\n",
      "It 1700: loss = 0.05135880038142204, accuracy = 1.0\n",
      "It 1800: loss = 0.10367917269468307, accuracy = 0.9545454545454546\n",
      "It 1900: loss = 0.123666912317276, accuracy = 0.9583333333333334\n",
      "It 2000: loss = 0.8650350570678711, accuracy = 0.8\n",
      "It 2100: loss = 0.4175436198711395, accuracy = 0.8205128205128205\n",
      "It 2200: loss = 0.28126072883605957, accuracy = 0.95\n",
      "It 2300: loss = 0.407452791929245, accuracy = 0.9090909090909091\n",
      "It 2400: loss = 0.019137876108288765, accuracy = 1.0\n",
      "It 2500: loss = 0.5345354080200195, accuracy = 0.8717948717948718\n",
      "It 2600: loss = 0.04839297756552696, accuracy = 1.0\n",
      "It 2700: loss = 0.42560654878616333, accuracy = 0.85\n",
      "It 2800: loss = 0.04433153197169304, accuracy = 0.9444444444444444\n",
      "It 2900: loss = 0.39563414454460144, accuracy = 0.8461538461538461\n",
      "It 3000: loss = 0.7950570583343506, accuracy = 0.8181818181818182\n",
      "It 3100: loss = 0.29282769560813904, accuracy = 0.95\n",
      "It 3200: loss = 0.07975007593631744, accuracy = 0.9655172413793104\n",
      "It 3300: loss = 0.030409162864089012, accuracy = 1.0\n",
      "It 3400: loss = 0.0719049796462059, accuracy = 1.0\n",
      "It 3500: loss = 0.5168465375900269, accuracy = 0.8157894736842105\n",
      "It 3600: loss = 0.3504129946231842, accuracy = 0.9285714285714286\n",
      "It 3700: loss = 0.11881835758686066, accuracy = 0.9090909090909091\n",
      "It 3800: loss = 0.11977703124284744, accuracy = 0.9666666666666667\n",
      "It 3900: loss = 0.20407786965370178, accuracy = 1.0\n",
      "It 4000: loss = 0.09679505974054337, accuracy = 0.9722222222222222\n",
      "It 4100: loss = 0.12411663681268692, accuracy = 1.0\n",
      "It 4200: loss = 0.09343504905700684, accuracy = 0.9523809523809523\n",
      "It 4300: loss = 0.48011207580566406, accuracy = 0.8461538461538461\n",
      "It 4400: loss = 0.17944686114788055, accuracy = 0.896551724137931\n",
      "It 4500: loss = 0.27205660939216614, accuracy = 0.8666666666666667\n",
      "It 4600: loss = 0.02517944946885109, accuracy = 1.0\n",
      "It 4700: loss = 0.18858645856380463, accuracy = 0.8666666666666667\n",
      "It 4800: loss = 0.4577489197254181, accuracy = 0.8478260869565217\n",
      "It 4900: loss = 0.13121117651462555, accuracy = 0.9285714285714286\n",
      "It 5000: loss = 0.2997235655784607, accuracy = 0.9047619047619048\n",
      "It 5100: loss = 0.16682013869285583, accuracy = 0.875\n",
      "It 5200: loss = 0.17723822593688965, accuracy = 0.9642857142857143\n",
      "It 5300: loss = 0.23415134847164154, accuracy = 0.9411764705882353\n",
      "It 5400: loss = 0.007361152675002813, accuracy = 1.0\n",
      "It 5500: loss = 0.07098303735256195, accuracy = 0.9545454545454546\n",
      "It 5600: loss = 0.04498695209622383, accuracy = 1.0\n",
      "It 5700: loss = 0.1042696088552475, accuracy = 0.9666666666666667\n",
      "It 5800: loss = 0.02747892588376999, accuracy = 1.0\n",
      "It 5900: loss = 0.2503018379211426, accuracy = 0.96\n",
      "It 6000: loss = 0.4205195903778076, accuracy = 0.8571428571428571\n",
      "It 6100: loss = 0.262531042098999, accuracy = 0.8846153846153846\n",
      "It 6200: loss = 0.25851547718048096, accuracy = 0.8837209302325582\n",
      "It 6300: loss = 0.1295596957206726, accuracy = 0.9459459459459459\n",
      "It 6400: loss = 0.13708758354187012, accuracy = 0.9655172413793104\n",
      "It 6500: loss = 0.1224595382809639, accuracy = 0.9318181818181818\n",
      "It 6600: loss = 0.20612086355686188, accuracy = 0.9333333333333333\n",
      "It 6700: loss = 0.3818168342113495, accuracy = 0.896551724137931\n",
      "It 6800: loss = 0.005261970218271017, accuracy = 1.0\n",
      "It 6900: loss = 0.24055629968643188, accuracy = 0.8333333333333334\n",
      "It 7000: loss = 0.11650765687227249, accuracy = 1.0\n",
      "It 7100: loss = 0.8289768099784851, accuracy = 0.8\n",
      "It 7200: loss = 0.35430818796157837, accuracy = 0.8636363636363636\n",
      "It 7300: loss = 0.053521979600191116, accuracy = 1.0\n",
      "It 7400: loss = 0.26974767446517944, accuracy = 0.8461538461538461\n",
      "It 7500: loss = 0.07789843529462814, accuracy = 0.9615384615384616\n",
      "It 7600: loss = 0.23947745561599731, accuracy = 1.0\n",
      "It 7700: loss = 0.00254741869866848, accuracy = 1.0\n",
      "It 7800: loss = 0.01052717212587595, accuracy = 1.0\n",
      "It 7900: loss = 0.42111772298812866, accuracy = 0.8888888888888888\n",
      "It 8000: loss = 0.2509949505329132, accuracy = 0.8823529411764706\n",
      "It 8100: loss = 0.07290289551019669, accuracy = 0.96\n",
      "It 8200: loss = 0.04098033159971237, accuracy = 1.0\n",
      "It 8300: loss = 0.2551305294036865, accuracy = 0.9285714285714286\n",
      "It 8400: loss = 0.6354953050613403, accuracy = 0.7714285714285715\n",
      "It 8500: loss = 0.04584486410021782, accuracy = 1.0\n",
      "It 8600: loss = 0.11698660999536514, accuracy = 0.9583333333333334\n",
      "It 8700: loss = 0.03539564460515976, accuracy = 1.0\n",
      "It 8800: loss = 0.10288631170988083, accuracy = 0.9333333333333333\n",
      "It 8900: loss = 0.21887581050395966, accuracy = 0.9166666666666666\n",
      "Epoch 0: train_accuracy = 0.9010817266765058\n",
      "Epoch 0: valid_accuracy = 0.9230672994243105\n"
     ]
    }
   ],
   "source": [
    "running_accuracy = 0\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    is_train = True\n",
    "    train_accuracy = 0.0\n",
    "    for idx, (sentence, tags) in enumerate(train_data):\n",
    "        # new training sequence -> zero the gradients of all models\n",
    "        model.zero_grad()\n",
    "\n",
    "        words_in = []\n",
    "        for word in sentence:\n",
    "            words_in.append(prepare_sequence(word, char_to_ix, is_train))\n",
    "\n",
    "        # one-hot encoding of words in sentence\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix, is_train)\n",
    "    \n",
    "        # compute the scores (forward pass)\n",
    "        tag_scores = model(sentence_in, words_in, is_train)\n",
    "    \n",
    "        # one-hot encoding of the labels for each word\n",
    "        targets = prepare_sequence(tags, tag_to_ix, is_train)\n",
    "    \n",
    "        # compute the accuracy\n",
    "        accuracy = sum(np.argmax(tag_scores.data.numpy(),1) == targets.data.numpy())/len(sentence)\n",
    "        train_accuracy += accuracy\n",
    "\n",
    "        # compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        losses.append(loss.data[0])\n",
    "    \n",
    "        if idx % PRINT_EVERY == 0:\n",
    "            print(\"It {}: loss = {}, accuracy = {}\".format(idx,loss.data[0],accuracy))\n",
    "\n",
    "    train_accuracy = train_accuracy / len(train_data)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(\"Epoch {}: train_accuracy = {}\".format(epoch, train_accuracy))\n",
    "\n",
    "    \n",
    "#     evaluate the validation accuracy after each epoch\n",
    "    is_train = False\n",
    "    test_accuracy = 0.0\n",
    "    for sentence, tags in test_data:\n",
    "    \n",
    "        # one-hot encoding of chars in words\n",
    "        words_in = []\n",
    "        for word in sentence:\n",
    "            words_in.append(prepare_sequence(word, char_to_ix, is_train))\n",
    "            \n",
    "        # one-hot encoding of words in sentence\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix, is_train)\n",
    "    \n",
    "        # compute the scores (forward pass)\n",
    "        tag_scores = model(sentence_in, words_in, is_train)\n",
    "    \n",
    "        # one-hot encoding of the labels for each word\n",
    "        targets = prepare_sequence(tags, tag_to_ix, is_train)\n",
    "    \n",
    "        # compute the accuracy\n",
    "        test_accuracy += sum(np.argmax(tag_scores.data.numpy(),1) == targets.data.numpy())/len(sentence)\n",
    "    \n",
    "    test_accuracy = test_accuracy / len(test_data)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    print(\"Epoch {}: test_accuracy = {}\".format(epoch, test_accuracy))\n",
    "    \n",
    "#     # save the best model so far\n",
    "#     if valid_accuracy.data[0] > best_valid_accuracy:\n",
    "#         torch.save(model.state_dict(), SAVE_PATH)\n",
    "#         best_valid_accuracy = valid_accuracy.data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
